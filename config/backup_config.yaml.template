# =====================================================================================
# PLANTILLA DE CONFIGURACIÓN - SISTEMA DE BACKUP INFLUXDB
# =====================================================================================
# Sistema de Backup Campo por Campo con Paralelización Configurable
#
# CARACTERÍSTICAS PRINCIPALES:
# ✅ Procesamiento campo por campo independiente
# ✅ Paralelización configurable (1-16+ workers)
# ✅ Prevención de contaminación cruzada
# ✅ Timestamps específicos por campo
# ✅ Thread-safe logging con identificadores únicos
# ✅ Métricas de eficiencia en tiempo real
#
# INSTRUCCIONES DE USO:
# 1. Copia este archivo: cp backup_config.yaml.template mi_backup.yaml
# 2. Personaliza la configuración según tus necesidades
# 3. Valida la configuración: python main.py --validate-only
# 4. Ejecuta el backup: python main.py --config /config
#
# DOCUMENTACIÓN COMPLETA: README.md
# =====================================================================================

# ===================================
# CONFIGURACIÓN GLOBAL DEL SISTEMA
# ===================================
global:
  # Red Docker para comunicación entre contenedores
  # IMPORTANTE: Todos los contenedores InfluxDB deben estar en esta red
  #
  # Para crear una red compartida:
  # docker network create influxdb_network
  #
  # Para conectar contenedores existentes:
  # docker network connect influxdb_network nombre_contenedor
  network: influxdb_network

# ========================================
# CONFIGURACIÓN SERVIDOR INFLUXDB ORIGEN
# ========================================
source:
  # URL del servidor InfluxDB de origen (OBLIGATORIO)
  #
  # FORMATOS VÁLIDOS:
  # - Contenedor Docker: http://nombre-contenedor:8086
  # - Servidor local: http://localhost:8086
  # - Servidor remoto: http://192.168.1.100:8086
  # - Con SSL: https://mi-servidor.com:8086
  #
  # EJEMPLOS:
  # url: http://sysadmintoolkit-influxdb-dev:8086    # Contenedor Docker
  # url: http://influxdb-production:8086             # Servidor producción
  # url: https://metrics.empresa.com:8086            # Servidor externo con SSL
  url: http://source-influxdb:8086

  # Configuración SSL/TLS
  ssl: false                    # true si usas HTTPS
  verify_ssl: true             # false para certificados autofirmados

  # ========================================
  # CONFIGURACIÓN DE BASES DE DATOS
  # ========================================
  #
  # OPCIÓN 1: Respaldar bases de datos específicas
  # Especifica exactamente qué bases de datos respaldar
  databases:
    - name: metrics             # Nombre en el servidor origen
      destination: metrics_backup  # Nombre en el servidor destino
    - name: telegraf
      destination: telegraf_backup
    - name: weather_data
      destination: weather_data_backup

  # OPCIÓN 2: Respaldar TODAS las bases de datos
  # Descomenta esta línea para respaldar todas las BD automáticamente:
  # databases: []

  # ========================================
  # PREFIJOS Y SUFIJOS PARA NOMBRES
  # ========================================
  # Útil para identificar bases de datos respaldadas
  #
  # EJEMPLOS:
  # prefix: "backup_"    → metrics se convierte en backup_metrics
  # suffix: "_bkp"       → metrics se convierte en metrics_bkp
  # prefix: "prod_", suffix: "_v2" → metrics se convierte en prod_metrics_v2
  prefix: ""
  suffix: ""

  # ========================================
  # AUTENTICACIÓN (Opcional)
  # ========================================
  # Deja en blanco si InfluxDB no requiere autenticación
  #
  # EJEMPLO CON AUTENTICACIÓN:
  # user: "admin"
  # password: "mi_password_seguro_123"
  user: ""
  password: ""

  # ========================================
  # AGRUPAMIENTO DE CONSULTAS
  # ========================================
  # Controla cómo se agrupan los datos en las consultas
  #
  # VALORES COMUNES:
  # - "30s": Agrupa datos cada 30 segundos (más consultas, menos memoria)
  # - "1m":  Agrupa datos cada minuto (balance)
  # - "5m":  Agrupa datos cada 5 minutos (menos consultas, más memoria)
  # - "1h":  Agrupa datos cada hora (para datos históricos grandes)
  # - "":    Sin agrupamiento (para datasets pequeños)
  #
  # RECOMENDACIONES:
  # - Datos en tiempo real: "30s" o "1m"
  # - Backups históricos: "5m" o "1h"
  # - Datasets pequeños: "" (sin agrupamiento)
  group_by: ""

# =========================================
# CONFIGURACIÓN SERVIDOR INFLUXDB DESTINO
# =========================================
destination:
  # URL del servidor InfluxDB de destino (OBLIGATORIO)
  # Puede ser el mismo servidor que el origen (usando diferentes BD)
  #
  # EJEMPLOS:
  # url: http://backup-influxdb:8086           # Servidor dedicado para backups
  # url: http://sysadmintoolkit-influxdb-dev:8086  # Mismo servidor, diferentes BD
  # url: http://192.168.1.200:8086            # Servidor de backup remoto
  url: http://destination-influxdb:8086

  # Configuración SSL/TLS
  ssl: false
  verify_ssl: true

  # Autenticación para el servidor destino (puede ser diferente al origen)
  user: ""
  password: ""

# =============================================
# CONFIGURACIÓN DE FILTRADO DE MEDICIONES
# =============================================
measurements:
  # ========================================
  # FILTRADO GLOBAL DE MEDICIONES
  # ========================================
  #
  # OPCIÓN 1: Incluir mediciones específicas (RECOMENDADO)
  # Solo las mediciones listadas serán respaldadas
  include: []
  # EJEMPLO: include: [cpu, memory, disk, network]

  # OPCIÓN 2: Excluir mediciones específicas
  # Todas las mediciones EXCEPTO las listadas serán respaldadas
  exclude: []
  # EJEMPLO: exclude: [debug_logs, temp_data, test_metrics]
  #
  # NOTA: No uses 'include' y 'exclude' al mismo tiempo

  # ========================================
  # CONFIGURACIONES ESPECÍFICAS POR MEDICIÓN
  # ========================================
  # Aquí defines el procesamiento detallado para cada medición
  # ¡ESTA ES LA CLAVE DEL SISTEMA CAMPO POR CAMPO!
  specific:
    # ========================================
    # EJEMPLO 1: PROCESAMIENTO DE MÉTRICAS CPU
    # ========================================
    # Scenario: Quieres solo métricas de uso de CPU, no todas las métricas
    cpu:
      fields:
        # Incluir solo campos específicos de CPU
        include: [usage_user, usage_system, usage_idle, usage_iowait]
        # Excluir campos que no necesitas
        exclude: []
        # Tipos de datos a procesar
        types: [numeric, string, boolean]

    # ========================================
    # EJEMPLO 2: PROCESAMIENTO DE MEMORIA
    # ========================================
    # Scenario: Quieres toda la memoria EXCEPTO campos de cache
    memory:
      fields:
        include: []  # Todos los campos
        # Excluir campos de cache que cambian constantemente
        exclude: [buffer, cached, slab]
        types: [numeric, string]

    # ========================================
    # EJEMPLO 3: DATOS METEOROLÓGICOS ESPECÍFICOS
    # ========================================
    # Scenario: Solo un campo específico de una medición grande
    ForecastingWeather:
      fields:
        # CAMPO POR CAMPO: Solo procesar temperatura
        include: [WRF_continuous_Temperature_2m_degC]
        exclude: []
        types: [numeric, string, boolean]

    # ========================================
    # EJEMPLO 4: MÚLTIPLES CAMPOS EN PARALELO
    # ========================================
    # Scenario: Varios campos de la misma medición procesados en paralelo
    WeatherStation:
      fields:
        # Múltiples campos que se procesarán independientemente
        include: [
          temperature,      # Thread T01
          humidity,         # Thread T02
          pressure,         # Thread T03
          wind_speed,       # Thread T04
          wind_direction,   # Thread T05
          rainfall,         # Thread T06
          solar_radiation,  # Thread T07
          uv_index         # Thread T08
        ]
        exclude: []
        types: [numeric]

    # ========================================
    # EJEMPLO 5: FILTRADO POR TIPO DE DATO
    # ========================================
    # Scenario: Solo quieres datos numéricos, no strings ni booleanos
    sensor_data:
      fields:
        include: []  # Todos los campos
        exclude: []
        types: [numeric]  # Solo datos numéricos

    # ========================================
    # EJEMPLO 6: LOGS Y EVENTOS
    # ========================================
    # Scenario: Procesar logs con datos de texto
    application_logs:
      fields:
        include: [level, message, timestamp, source]
        exclude: [debug_info, stack_trace]
        types: [string, boolean]  # Solo texto y flags

# =============================================
# OPCIONES DE BACKUP Y PARALELIZACIÓN
# =============================================
options:
  # ========================================
  # MODO DE BACKUP
  # ========================================
  #
  # INCREMENTAL: Copia solo datos nuevos desde la última ejecución
  # - Ideal para: Backups continuos, datos en tiempo real
  # - Usa: Timestamps específicos por campo para evitar contaminación
  # - Ejecuta: Según schedule configurado
  #
  # RANGE: Copia datos de un período específico
  # - Ideal para: Backups históricos, migraciones de datos
  # - Usa: Fechas fijas de inicio y fin
  # - Ejecuta: Una sola vez y termina
  backup_mode: incremental

  # ========================================
  # CONFIGURACIÓN MODO RANGE
  # ========================================
  # Solo se usa si backup_mode = "range"
  range:
    # Fechas en formato ISO 8601 (YYYY-MM-DDTHH:MM:SSZ)
    #
    # EJEMPLOS:
    # start_date: "2023-01-01T00:00:00Z"    # Año completo
    # end_date: "2023-12-31T23:59:59Z"
    #
    # start_date: "2023-12-01T08:00:00Z"    # Un día específico
    # end_date: "2023-12-01T20:00:00Z"
    start_date: "2023-01-01T00:00:00Z"
    end_date: "2023-12-31T23:59:59Z"

  # ========================================
  # CONFIGURACIÓN MODO INCREMENTAL
  # ========================================
  # Solo se usa si backup_mode = "incremental"
  incremental:
    # Expresión CRON para programar ejecuciones automáticas
    # Deja vacío para ejecutar solo una vez
    #
    # EJEMPLOS DE EXPRESIONES CRON:
    # "* * * * *"        # Cada minuto (para testing)
    # "*/5 * * * *"      # Cada 5 minutos
    # "0 * * * *"        # Cada hora
    # "0 */6 * * *"      # Cada 6 horas
    # "0 0 * * *"        # Diario a medianoche
    # "0 2 * * 0"        # Semanal los domingos a las 2 AM
    # "0 1 1 * *"        # Mensual el día 1 a la 1 AM
    #
    # FORMATO: "minuto hora día_mes mes día_semana"
    schedule: ""

  # ========================================
  # CONFIGURACIÓN DE CONEXIONES
  # ========================================
  # Timeouts y reintentos para conexiones de red
  timeout_client: 20                    # Timeout por consulta (segundos)
  retries: 3                           # Reintentos en caso de error
  retry_delay: 5                       # Espera entre reintentos (segundos)
  initial_connection_retry_delay: 60   # Espera para reconectar al inicio

  # ========================================
  # CONFIGURACIÓN DE PAGINACIÓN
  # ========================================
  # Divide consultas grandes en chunks para evitar problemas de memoria
  #
  # VALORES RECOMENDADOS:
  # - 1 día: Para bases de datos muy grandes (>100GB)
  # - 7 días: Para bases de datos medianas (10-100GB)
  # - 30 días: Para bases de datos pequeñas (<10GB)
  days_of_pagination: 7

  # =========================================
  # CONFIGURACIÓN DE PARALELIZACIÓN
  # =========================================
  # ¡NUEVA FUNCIONALIDAD PRINCIPAL!
  # Número de hilos para procesar campos en paralelo
  #
  # CÓMO FUNCIONA:
  # - Cada campo se procesa en su propio hilo independiente
  # - Timestamps específicos por campo (NO contaminación cruzada)
  # - ThreadPoolExecutor gestiona la concurrencia
  # - Logging con identificadores únicos por thread ([T01], [T02], etc.)
  #
  # GUÍA DE CONFIGURACIÓN:
  #
  # ┌─────────────────┬──────────────────┬─────────────────────────────────┐
  # │ CAMPOS A BACKUP │ PARALLEL_WORKERS │ DESCRIPCIÓN                     │
  # ├─────────────────┼──────────────────┼─────────────────────────────────┤
  # │ 1 campo         │ 1                │ Secuencial (máxima seguridad)   │
  # │ 2-4 campos      │ 2-4              │ Paralelismo completo            │
  # │ 5-8 campos      │ 4-6              │ Balance rendimiento/recursos    │
  # │ 9+ campos       │ 6-8              │ Alto rendimiento                │
  # │ Servidor potente│ 8-16             │ Máximo paralelismo              │
  # └─────────────────┴──────────────────┴─────────────────────────────────┘
  #
  # EJEMPLOS POR ESCENARIO:
  #
  # EJEMPLO A: Un solo campo crítico
  # parallel_workers: 1
  # measurements.specific.CriticalSensor.fields.include: [sensor_reading]
  #
  # EJEMPLO B: Estación meteorológica (8 sensores)
  # parallel_workers: 8
  # measurements.specific.Weather.fields.include: [temp, humidity, pressure, ...]
  #
  # EJEMPLO C: Múltiples mediciones con pocos campos cada una
  # parallel_workers: 4  # Balance general
  #
  # IMPORTANTE:
  # - No exceder número de CPU cores disponibles
  # - Monitorear uso de memoria con parallel_workers alto
  # - Para testing inicial usar parallel_workers: 1
  parallel_workers: 4

  # ========================================
  # GESTIÓN DE CAMPOS OBSOLETOS
  # ========================================
  # Evita procesar campos que ya no reciben datos nuevos
  #
  # FORMATO: número + unidad (s, m, h, d, w, M, y)
  # EJEMPLOS:
  # "30d"  - Ignorar campos sin datos en 30 días
  # "6M"   - Ignorar campos sin datos en 6 meses
  # "1y"   - Ignorar campos sin datos en 1 año
  # ""     - No filtrar por obsolescencia (procesar todo)
  #
  # RECOMENDADO: "6M" para la mayoría de casos
  field_obsolete_threshold: "6M"

  # ========================================
  # CONFIGURACIÓN DE LOGGING
  # ========================================
  log_directory: /var/log/backup_influxdb/

  # Rotación automática de logs
  log_rotation:
    enabled: true
    when: 'D'        # D=diario, H=por hora, M=por minuto
    interval: 1      # Cada cuánto rotar (1 día)
    backup_count: 7  # Mantener 7 archivos históricos

  # Configuración Loki (logging centralizado)
  loki:
    enabled: true
    url: "sysadmintoolkit-loki-dev"  # Servidor Loki
    port: 3100
    # IMPORTANTE: Usar tags únicos para distinguir configuraciones
    tags:
      app: "influxdb-backup"
      environment: "production"
      config: "mi_backup.yaml"  # ¡CAMBIAR POR NOMBRE REAL DEL ARCHIVO!

  # Nivel de detalle de logs
  # DEBUG: Muy detallado (para desarrollo)
  # INFO: Normal (recomendado para producción)
  # WARNING: Solo advertencias y errores
  # ERROR: Solo errores críticos
  log_level: INFO

# =============================================
# EJEMPLOS DE CONFIGURACIONES COMPLETAS
# =============================================

# ========================================
# CONFIGURACIÓN TIPO A: CAMPO ÚNICO CRÍTICO
# ========================================
# Use case: Backup de un sensor crítico específico
#
# measurements:
#   include: [CriticalSensors]
#   specific:
#     CriticalSensors:
#       fields:
#         include: [reactor_temperature]  # Solo temperatura del reactor
#
# options:
#   backup_mode: incremental
#   parallel_workers: 1                  # Secuencial para máxima seguridad
#   incremental:
#     schedule: "* * * * *"               # Cada minuto
#   field_obsolete_threshold: ""         # No filtrar por obsolescencia

# ========================================
# CONFIGURACIÓN TIPO B: ESTACIÓN METEOROLÓGICA
# ========================================
# Use case: Múltiples sensores independientes
#
# measurements:
#   include: [WeatherStation]
#   specific:
#     WeatherStation:
#       fields:
#         include: [
#           temperature,     # T01
#           humidity,        # T02
#           pressure,        # T03
#           wind_speed,      # T04
#           wind_direction,  # T05
#           rainfall,        # T06
#           uv_index,        # T07
#           solar_radiation  # T08
#         ]
#
# options:
#   backup_mode: incremental
#   parallel_workers: 8                  # Un worker por sensor
#   incremental:
#     schedule: "*/5 * * * *"             # Cada 5 minutos

# ========================================
# CONFIGURACIÓN TIPO C: BACKUP HISTÓRICO MASIVO
# ========================================
# Use case: Migrar datos históricos de un año completo
#
# measurements:
#   include: []                          # Todas las mediciones
#
# options:
#   backup_mode: range
#   parallel_workers: 16                 # Máximo paralelismo
#   range:
#     start_date: "2023-01-01T00:00:00Z"
#     end_date: "2023-12-31T23:59:59Z"
#   days_of_pagination: 1                # Chunks pequeños para seguridad

# ========================================
# CONFIGURACIÓN TIPO D: MÚLTIPLES MEDICIONES
# ========================================
# Use case: Varias mediciones con campos específicos cada una
#
# measurements:
#   include: [cpu, memory, disk, network]
#   specific:
#     cpu:
#       fields:
#         include: [usage_user, usage_system, usage_idle]
#     memory:
#       fields:
#         include: [used, available, percent]
#     disk:
#       fields:
#         include: [used_percent, free]
#     network:
#       fields:
#         include: [bytes_sent, bytes_recv]
#
# options:
#   parallel_workers: 6                  # Balance para múltiples mediciones
#   incremental:
#     schedule: "0 */6 * * *"             # Cada 6 horas

# =============================================
# PASOS PARA CONFIGURAR TU BACKUP
# =============================================
#
# 1. COPIA ESTE TEMPLATE:
#    cp backup_config.yaml.template mi_configuracion.yaml
#
# 2. CONFIGURA LOS SERVIDORES:
#    - Ajusta las URLs de source y destination
#    - Configura autenticación si es necesaria
#
# 3. DEFINE QUÉ RESPALDAR:
#    - Especifica bases de datos en source.databases
#    - Configura mediciones en measurements.include/exclude
#    - Define campos específicos en measurements.specific
#
# 4. CONFIGURA PARALELIZACIÓN:
#    - Cuenta cuántos campos vas a procesar
#    - Configura parallel_workers apropiadamente
#    - Para empezar usa parallel_workers: 1
#
# 5. CONFIGURA HORARIOS:
#    - Para backup continuo: incremental.schedule con cron
#    - Para backup único: range con fechas específicas
#
# 6. PERSONALIZA LOGGING:
#    - Cambia loki.tags.config al nombre de tu archivo
#    - Ajusta log_level según necesidades
#
# 7. VALIDA LA CONFIGURACIÓN:
#    python main.py --validate-only
#
# 8. EJECUTA EL BACKUP:
#    python main.py --config /config --verbose
#
# 9. MONITOREA LOS LOGS:
#    tail -f /var/log/backup_influxdb/mi_configuracion.yaml/backup.log
#
# 10. REVISA MÉTRICAS:
#    grep "Parallelization metrics" logs/*.log

# =============================================
# ¿NECESITAS AYUDA?
# =============================================
#
# 📖 Documentación completa: README.md
# 🔧 Configuraciones ejemplo: config/irr.yaml, config/temp.yaml
# 🐛 Troubleshooting: Ver sección "Troubleshooting" en README.md
# 📊 Métricas: Dashboard Grafana en http://localhost:3000
#
# COMANDO ÚTILES:
# - Validar: python main.py --validate-only
# - Ejecutar: python main.py --config /config
# - Debug: python main.py --config /config --verbose
# - Ver logs: tail -f /var/log/backup_influxdb/*/backup.log
#
# =============================================
